# 最好、最坏、平均、均摊时间复杂度

本文主要介绍：

1. 最好情况时间复杂度（best case time complexity）
2. 最坏情况时间复杂度（worst case time complexity）
3. 平均情况时间复杂度（average case time complexity）
4. 均摊时间复杂度（amortized time complexity）

## 1 最好、最坏情况时间复杂度

来看下面代码：

```java
int find(int[] array, int n, int x) {
    int i = 0;
    int pos = -1;
    for (; i < n; ++i) {
        if (array[i] == x) pos = i;
    }
    return pos;
}
```

这段代码要实现的功能是：在一个无序数组 `array` 中，查找变量 `x` 出现的位置，如果没有找到则返回 `-1`。根据之前推理，我们可以得出这段代码的时间复杂度是 `O(n)`。

但是，这段代码并不高效，当我们中途找到的时候，就不需要继续遍历了，所以可以稍作优化：

```java
int find(int[] array, int n, int x) {
    int i = 0;
    int pos = -1;
    for (; i < n; ++i) {
        if (array[i] == x){
            pos = i;
            break;
        }
    }
    return pos;
}
```

这个时候的时间复杂度还是 `O(n)` 吗？如果在第 1 个便找到并返回了，那么时间复杂度就是 `O(1)`，如果 `x` 不存在数组中，那么时间复杂度就是 `O(n)`。由此可见，不同情况下这段代码的时间复杂度并不相同。

为了表示代码在不同情况下不同的时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度。

最好情况时间复杂度就是在最理想的情况下，执行这段代码的时间复杂度。最坏情况时间复杂度就是在最糟糕的情况下，执行这段代码的时间复杂度。

## 2 平均情况时间复杂度


最好和最坏时间复杂度都是在极端的情况下产生的，概率并不大。为了更好地表示平均情况下的复杂度，我们引入平均时间复杂度。

平均复杂度该怎么分析呢？我们还是借助上面例子：

要查找变量 `x` 在数组中的位置，一共有 `n+1` 种情况：在数组 0 - `n-1` 处和不在数组中。我们把每种情况下，查找需要遍历的元素个数加起来，然后再除以 `n+1`，就可以得到需要遍历的元素个数的平均值，即：

```
(1+2+3+ ··· +n+n) / (n+1) = n(n+3) / 2(n+1)
```

而大 O 时间复杂度表示法中可以忽略系数、低阶、常量，所以，上述公式简化之后，得到的时间复杂度就是 `O(n)`。

这个结论虽然是正确的，但是计算过程稍微有点问题：我们刚讲的 `n+1` 种情况，出现的概率并不一样。我们知道，要查找的变量 `x`，要么在数组里，要么就不在数组里，这两种情况对应的概率统计起来很麻烦，为了方便理解，我们假设这两种情况概率都为 1/2。另外，要查找的数据出现在 0 ~ n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的数据出现在 0 ~ n-1 中任意位置的概率就是 1/(2n)。

因此，前面的推导过程中存在的最大问题就是，没有将各种情况发生的概率考虑进去。如果我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了：

```
1 * 1/(2n) + 2 * 1/(2n) + 3 * 1/(2n) + ··· + n * 1/(2n) + n * 1/2 = (3n+1)/4
```

这个值就是概率论中的**加权平均值**，也叫做**期望值**，所以平均时间复杂度的全称应该是**加权平均时间复杂度**或者**期望时间复杂度**。

引入加权平均值后，再根据大 O 表示法去掉系数和常量，这段代码的加权平均时间复杂度仍然是 `O(n)`。

> 很多时候，我们使用一个复杂度就可以满足需求了。只有同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。

## 3 均摊时间复杂度

均摊时间复杂度应用的场景比平均时间复杂度更加特殊、有限。

看一个特殊的例子：

```java
/**
 * array 表示一个长度为 n 的数组
 */
int[] array = new int[n]
int count = 0

void insert(int val) {
    if (count == array.length) {
        int sum = 0;
        for (int i = 0; i < array.length; ++i) {
            sum = sum + array[i];
        }

        array[0] = sum;
        count = 1;
    }

    array[count] = val;
    ++count;
} 
```

这段代码主要实现了往数组里插入数据的功能。当数组内元素个数达到上限之后，则将所有的元素相加并放在数组的第一位。

下面我们分析这段代码的时间复杂度。先用最好、最坏、平均时间复杂度来分析：

**最好**的情况下，数组中有空闲位置，可以直接插入到下标为 `count` 的位置，所以最好时间复杂度是 `O(1)`。如果数组中没有空间了，那么需要遍历一遍数组进行加和操作，**最坏**的时间复杂度是 `O(n)`。

**平均**时间复杂度是 `O(1)`。假设数组的长度是 `n`，根据数据插入的位置，我们可以分成 `n` 种情况，每种情况的时间复杂度是 `O(1)`。除此之外，还有一种情况，就是数组无剩余空间时，这个时候的时间复杂度是 `O(n)`，而且，这 `n+1` 种情况发生的概率一样，都是 `1/(n+1)`。所以，根据加权平均的计算方法，平均时间复杂度就是：

```
1 * 1/(n+1) + 1 * 1/(n+1) + ··· + 1 * 1/(n+1) + n * 1/(n+1) = O(1)
```

至此为止，前面的最好、最坏、平均时间复杂度的计算，理解起来应该都没有问题。但是，这个例子其实并不需要这么复杂地分析，我们可以对比一下 `insert()` 和 `find()`：

+ 首先 `find()` 函数只有在极端的情况下，复杂度才为 `O(1)`。但 `insert` 在大部分情况下，时间复杂度都为 `O(1)`。只有在个别情况下，复杂度才比较高，为 `O(n)`。这是 `insert()` 第一个区别于 `find()` 的地方。

+ 第二个不同的地方在于 `insert()` 函数 `O(1)` 时间复杂度的插入和 `O(n)` 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时许关系，一般都是一个 `O(n)` 插入之后，紧跟着 `n-1` 个 `O(1)` 的插入操作，循环往复。

所以，针对 `insert()` 函数这种特殊场景的复杂度分析，我们并不需要像求平均时间复杂度那样找出所有的情况及其发生概率，然后在计算加权平均值。

针对这种情况，我们引入**均摊时间复杂度**。每一次 `O(n)` 的插入操作，都会跟着 `n-1` 次 `O(1)` 的插入操作，所以把耗时多的那次操作均摊到剩下的 `n-1` 次操作上，均摊下来这一组连续的操作的时间复杂度就是 `O(1)`。这是均摊分析的大致思路。

均摊时间复杂度的使用场景比较特殊，总结来说大致如下：

对一个数据结构进行连续一组操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这组操作放在一块儿分析，看是否能将较高时间复杂度的那次操作均摊到其他操作时间较低的情况。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好时间复杂度。