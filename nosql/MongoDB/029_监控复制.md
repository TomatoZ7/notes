# 监控复制

监控副本集的状态非常重要：不仅要监控是否所有成员都可用，也要监控每个成员处于什么状态，以及每个成员的数据新旧程度。

与复制相关的故障通常都是很短暂的：一个服务器刚才还连接不到另一个服务器，但是现在又可以连上了。要查看这样的问题，**最简单的方式就是查看日志**。确保自己知道日志的保存位置（而且真的被保存下来），确保能够访问到它们。

## 1 获取状态

`replSetGetStatus` 可以返回副本集中每个成员的当前信息。这个命令还有一个对应的辅助函数 `rs.status`：

```shell
$ rs.status()
{
	"set" : "rs0",
	"date" : ISODate("2021-10-17T10:36:54.394Z"),
	"myState" : 1,
	"syncSourceHost" : "",
	"syncSourceId" : -1,
	"members" : [
		{
			"_id" : 0,
			"name" : "server:28017",
			"health" : 1,
			"state" : 1,
			"stateStr" : "PRIMARY",
			"uptime" : 849055,
			"optime" : {
				"ts" : Timestamp(1634467012, 1),
				"t" : NumberLong(1)
			},
			"optimeDate" : ISODate("2021-10-17T10:36:52Z"),
			"syncSourceHost" : "",
			"syncSourceId" : -1,
			"infoMessage" : "",
			"electionTime" : Timestamp(1633618347, 2),
			"electionDate" : ISODate("2021-10-07T14:52:27Z"),
			"configVersion" : 5,
			"configTerm" : 1,
			"self" : true,
			"lastHeartbeatMessage" : ""
		},
		{
			"_id" : 1,
			"name" : "server:28018",
			"health" : 1,
			"state" : 2,
			"stateStr" : "SECONDARY",
			"uptime" : 848632,
			"optime" : {
				"ts" : Timestamp(1634467012, 1),
				"t" : NumberLong(1)
			},
			"optimeDurable" : {
				"ts" : Timestamp(1634467012, 1),
				"t" : NumberLong(1)
			},
			"optimeDate" : ISODate("2021-10-17T10:36:52Z"),
			"optimeDurableDate" : ISODate("2021-10-17T10:36:52Z"),
			"lastHeartbeat" : ISODate("2021-10-17T10:36:54.385Z"),
			"lastHeartbeatRecv" : ISODate("2021-10-17T10:36:52.560Z"),
			"pingMs" : NumberLong(0),
			"lastHeartbeatMessage" : "",
			"syncSourceHost" : "server:28017",
			"syncSourceId" : 0,
			"infoMessage" : "",
			"configVersion" : 5,
			"configTerm" : 1
		},
		{
			"_id" : 2,
			"name" : "server:28019",
			"health" : 1,
			"state" : 2,
			"stateStr" : "SECONDARY",
			"uptime" : 848626,
			"optime" : {
				"ts" : Timestamp(1634467012, 1),
				"t" : NumberLong(1)
			},
			"optimeDurable" : {
				"ts" : Timestamp(1634467012, 1),
				"t" : NumberLong(1)
			},
			"optimeDate" : ISODate("2021-10-17T10:36:52Z"),
			"optimeDurableDate" : ISODate("2021-10-17T10:36:52Z"),
			"lastHeartbeat" : ISODate("2021-10-17T10:36:53.362Z"),
			"lastHeartbeatRecv" : ISODate("2021-10-17T10:36:54.258Z"),
			"pingMs" : NumberLong(0),
			"lastHeartbeatMessage" : "",
			"syncSourceHost" : "server:28018",
			"syncSourceId" : 1,
			"infoMessage" : "",
			"configVersion" : 5,
			"configTerm" : 1
		}
	],
	"ok" : 1
}
```

+ **self**

这个字段只会出现在执行 `rs.status` 的成员信息中，在本例中是 28017 端口。

+ **stateStr**

用于描述服务器状态的字符串。

+ **uptime**

从成员可达一直到现在所经历的时间，单位是秒。如果是 `self` 成员，这个值是从它启动直到现在的时间。

+ **optimeDate**

每个成员的 `oplog` 中最后一个操作发生的时间（也就是操作被同步过来的时间）。注意，这里的状态是每个成员通过心跳报告上来的状态，所以 `optime` 跟实际时间可能会有几秒钟的偏差。

+ **lastHeartbeat**

当前服务器最后一次收到其他成员心跳的时间。如果网络故障或者当前服务器比较繁忙，这个时间可能会是 2 秒钟之前。

+ **pingMs**

心跳从当前服务器到达某个成员所花费的平均时间，可以根据这个字段选择从哪个成员进行同步。

有几个字段的信息是重复的：`state` 与 `stateStr` 都表示成员的状态，只是 `state` 的值是状态的内部表示法。

`health` 仅仅表示给定的服务器是否可达，而从 `state` 和 `stateStr` 也可以得到这样的信息（如果服务器不可达，它们的值会是 `UNKNOWN` 或者 `DOWN`）。

类似地，`optime` 和 `optimeDate` 的值也是相同的，只是表示方式不同：一个是用从新纪元开始的毫秒数表示的，另一个用一种更适合阅读的方式表示。

注意，这份报告是以执行 `rs.status()` 命令的成员的角度得出的：由于网络故障，这份报告可能不准确或者有些过时。

## 2 复制图谱

如果在备份节点上运行 `rs.status()`，输出信息中会有一个名为 `syncingTo` 的顶级字段，用于表示当前成员正在从哪个成员处进行复制。

如果在每个成员上运行 `replSetGetStatus` 命令，就可以弄清楚**复制图谱（replication graph）**。假设 `server1` 表示连接到 `server1` 的数据库连接，`server2` 表示连接到 `server2` 的数据库连接，以此类推，然后分别在这些连接上执行下面的命令：

```shell
$ server1.adminCommand({replSetGetStatus: 1})['syncingTo']
server0:27017
$ server2.adminCommand({replSetGetStatus: 1})['syncingTo']
server1:27017
$ server3.adminCommand({replSetGetStatus: 1})['syncingTo']
server1:27017
$ server4.adminCommand({replSetGetStatus: 1})['syncingTo']
server2:27017
```

所以，`server0` 是 `server1` 的同步源，`server1` 是 `server2` 和 `server3` 的同步源，`server2` 是 `server4` 的同步源。

`MongoDB` 根据 `ping` 时间选择同步源。一个成员向另一个成员发送心跳请求，就可以知道心跳请求所耗费的时间。`MongoDB` 维护着不同成员间请求的平均花费时间。选择同步源时，会选择一个离自己比较近而且数据比自己新的成员（所以，不会出现循环复制的问题，每个成员要么从主节点复制，要么从数据比它新的成员处复制）。

因此，如果在备份数据中心中添加一个新成员，它很可能会从与自己同在一个数据中心内的其他成员处复制，而不是从位于另一个数据中心的主节点处复制（这样可以减少网络流量），如图所示。

![image](https://github.com/TomatoZ7/notes-of-tz/blob/master/nosql/MongoDB/images/mongo_monitor_replica_1.jpg)

但是，**自动复制链（automatic replication chaining）**也有一些缺点：复制链越长，将写操作复制到所有服务器所花费的时间就越长。假设所有服务器都位于同一个数据中心内，然后，由于网络速度异常，新添加一个成员之后，`MongoDB` 的复制链如图所示。

![image](https://github.com/TomatoZ7/notes-of-tz/blob/master/nosql/MongoDB/images/mongo_monitor_replica_2.jpg)

通常不会发生这样的情况，只要出现这种状况，可以用 `replSetSyncFrom` 或者 `rs.syncForm()` 命令修改成员的复制源：

```shell
$ secondary.adminCommand({"replSetSyncForm": "server0:27017"})
```

## 3 复制循环

**如果复制链中出现了环，那么就成为发生了复制循环。**

例如，A 从 B 处同步数据，B 从 C 处同步数据，C 从 A 处同步数据，这就是一个复制循环。因为复制循环中的成员都不可能成为主节点，所以这些成员无法复制新的写操作，就会越来越落后。另一方面，**如果每个成员都是自动选取复制源，那么复制循环是不可能发生的**。

但是，使用 `replSetSyncFrom` 强制为成员设置复制源时，就可能会出现复制循环。在手动修改成员的复制源时，应该仔细查看 `rs.status()` 的输出信息，避免造成复制循环。当用 `replSetSyncFrom` 为成员指定一个并不比它领先的成员作为复制源时，系统会给出警告，但仍然允许这么做。

## 4 禁用复制链

当一个备份节点从另一个备份节点（而不是主节点）复制数据时，就会形成复制链。前面说过，成员会自动选择其他成员作为复制源。

可以禁用复制链，强制要求每个成员都从主节点进行复制，只需要将 `allowChaining` 设置为 `false` 即可（如果不指定这个选项，默认是 `true`）：

```shell
$ var config = rs.config()
$ config.settings = config.settings || {}
$ config.settings.allowChaining = false
$ re.reconfig(config)
```

如果主节点变得不可用，那么各个成员就会从其他备份节点处复制数据。

## 5 计算延迟

跟踪复制情况的一个重要指标是备份节点与主节点之间的延迟程度。**延迟（lag）是指备份节点相对于主节点的落后程度，是主节点最后一次操作的时间戳与备份节点最后一次操作的时间戳的差。**

可以使用 `rs.status()` 查看成员的复制状态，也可以通过在主节点上执行 `db.printReplicationInfo()`（这个命令的输出信息中包含 `oplog` 相关信息），或者在备份节点上执行 `db.printSecondaryReplicationInfo()` 快速得到一份摘要。注意，这两个都是 `db` 的函数，而不是 `rs` 的。

`db.printReplicationInfo` 的输出中包括主节点的 `oplog` 信息：

```shell
$ db.printReplicationInfo()
configured oplog size:   2503.301513671875MB
log length start to end: 950409secs (264hrs)
oplog first event time:  Thu Oct 07 2021 22:52:27 GMT+0800 (CST)
oplog last event time:   Mon Oct 18 2021 22:52:36 GMT+0800 (CST)
now:                     Mon Oct 18 2021 22:52:36 GMT+0800 (CST)
```

上面的输出信息中包含了 `oplog` 的大小，以及 `oplog` 中包含的操作的时间范围。在本例中，`oplog` 的大小大约是 2503MB，包含大约 12 天的操作。

`oplog` 中第一条操作与最后一条操作的时间差就是操作日志的长度。

在备份节点上运行 `db.printSecondaryReplicationInfo()`，可以得到当前成员的复制源，以及当前成员相对复制源的落后程度等信息：

```shell
$ db.printSecondaryReplicationInfo()
source: server-0:28018
	syncedTo: Tue Oct 19 2021 21:33:29 GMT+0800 (CST)
	0 secs (0 hrs) behind the primary 
source: server-0:28019
	syncedTo: Tue Oct 19 2021 21:33:29 GMT+0800 (CST)
	0 secs (0 hrs) behind the primary
```

注意，副本集成员的延迟是**相对于主节点**来说的，而不是表示需要多长时间才能更新到最新。在一个写操作非常少的系统中，有可能会造成延迟过大的幻觉。假设一小时执行一次写操作。刚刚执行完这次写操作之后，复制之前，备份节点会落后于主节点一小时。但是，只需要几毫秒时，备份节点就可以追上主节点。当监控低吞吐量的系统时，这个值可能会造成迷惑。

## 6 调整 oplog 大小

如果要增加 `oplog` 大小，可以按照如下步骤：

1. 如果当前服务器是主节点，让它退位，以便让其他成员的数据能够尽快更新到与它一致。

2. 关闭当前服务器。

3. 将当前服务器以单机模式启动。

4. 临时将 `oplog` 中的最后一条 `insert` 操作保存到其他集合中：

```shell
$ use local
# {"op": "i"} 用于查找最后一条 insert 操作
$ var cursor = db.oplog.rs.find({"op": "i"})

$ var lastInsert = cursor.sort({"$natural": -1}).limit(1).next()
$ db.tempLastOp.save(lastInsert)

# 确保保存成功，非常重要
$ db.tempLastOp.findOne()
```

5. 删除当前的 `oplog`

```shell
$ db.oplog.rs.drop()
```

6. 创建一个新的 `oplog`：

```shell
$ db.createCollection("oplog.rs", {"capped": true, "size": 10000})
```

7. 将最后一条记录写回 `oplog`：

```shell
$ var temp = db.tempLastOp.findOne()
$ db.oplog.rs.insert(temp)
# 确保插入成功，否则把当前服务器添加到副本集之后，它会删除所有数据，然后重新进行一次完整同步。
$ db.oplog.rs.findOne()
```

8. 最后，将当前服务器作为副本及成员重新启动。由于这时它的 `oplog` 只有一条记录，所以在一段时间内无法知道 `oplog` 的真实长度。另外，这个服务器现在也并不适合作为其他成员的复制源。

通常不应该减小 `oplog` 的大小：即使 `oplog` 可能会有几个月那么长，但是通常总是有足够的硬盘空间来保存 `oplog`，`oplog` 并不会占用任何珍贵的资源（比如 CPU 或 RAM）。

## 7 从延迟备份节点中恢复

假设有人不小心删除了一个数据库，幸好你有一个延迟备份节点。现在，需要放弃其他成员的数据，明确将延迟备份节点指定为数据源。

下面介绍最简单的方法。

1. 关闭所有其他成员。

2. 删除其他成员数据目录中的所有数据。确保每个成员（除了延迟备份节点）的数据目录都是空的。

3. 重启所有成员，然后它们会自动从延迟备份节点中复制数据。

这种方式非常简单。但是，在其他成员完成初始化同步之前，副本集中将**只有一个成员可用（延迟备份节点）**而且这个成员很可能会过载。

根据数据量的不同，第二种方式可能更好，也可能更差。

1. 关闭所有成员，包括延迟备份节点。

2. 删除其他成员（除了延迟备份节点）的数据目录。

3. 将延迟备份节点的数据文件复制到其他服务器。

4. 重启所有成员。

注意，这样会导致所有服务器都与延迟备份节点拥有同样大小的 `oplog`，这可能不是你想要的。