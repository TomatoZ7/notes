# 监控复制

监控副本集的状态非常重要：不仅要监控是否所有成员都可用，也要监控每个成员处于什么状态，以及每个成员的数据新旧程度。

与复制相关的故障通常都是很短暂的：一个服务器刚才还连接不到另一个服务器，但是现在又可以连上了。要查看这样的问题，**最简单的方式就是查看日志**。确保自己知道日志的保存位置（而且真的被保存下来），确保能够访问到它们。

## 1 获取状态

`replSetGetStatus` 可以返回副本集中每个成员的当前信息。这个命令还有一个对应的辅助函数 `rs.status`：

```shell
$ rs.status()
{
	"set" : "rs0",
	"date" : ISODate("2021-10-17T10:36:54.394Z"),
	"myState" : 1,
	"syncSourceHost" : "",
	"syncSourceId" : -1,
	"members" : [
		{
			"_id" : 0,
			"name" : "server:28017",
			"health" : 1,
			"state" : 1,
			"stateStr" : "PRIMARY",
			"uptime" : 849055,
			"optime" : {
				"ts" : Timestamp(1634467012, 1),
				"t" : NumberLong(1)
			},
			"optimeDate" : ISODate("2021-10-17T10:36:52Z"),
			"syncSourceHost" : "",
			"syncSourceId" : -1,
			"infoMessage" : "",
			"electionTime" : Timestamp(1633618347, 2),
			"electionDate" : ISODate("2021-10-07T14:52:27Z"),
			"configVersion" : 5,
			"configTerm" : 1,
			"self" : true,
			"lastHeartbeatMessage" : ""
		},
		{
			"_id" : 1,
			"name" : "server:28018",
			"health" : 1,
			"state" : 2,
			"stateStr" : "SECONDARY",
			"uptime" : 848632,
			"optime" : {
				"ts" : Timestamp(1634467012, 1),
				"t" : NumberLong(1)
			},
			"optimeDurable" : {
				"ts" : Timestamp(1634467012, 1),
				"t" : NumberLong(1)
			},
			"optimeDate" : ISODate("2021-10-17T10:36:52Z"),
			"optimeDurableDate" : ISODate("2021-10-17T10:36:52Z"),
			"lastHeartbeat" : ISODate("2021-10-17T10:36:54.385Z"),
			"lastHeartbeatRecv" : ISODate("2021-10-17T10:36:52.560Z"),
			"pingMs" : NumberLong(0),
			"lastHeartbeatMessage" : "",
			"syncSourceHost" : "server:28017",
			"syncSourceId" : 0,
			"infoMessage" : "",
			"configVersion" : 5,
			"configTerm" : 1
		},
		{
			"_id" : 2,
			"name" : "server:28019",
			"health" : 1,
			"state" : 2,
			"stateStr" : "SECONDARY",
			"uptime" : 848626,
			"optime" : {
				"ts" : Timestamp(1634467012, 1),
				"t" : NumberLong(1)
			},
			"optimeDurable" : {
				"ts" : Timestamp(1634467012, 1),
				"t" : NumberLong(1)
			},
			"optimeDate" : ISODate("2021-10-17T10:36:52Z"),
			"optimeDurableDate" : ISODate("2021-10-17T10:36:52Z"),
			"lastHeartbeat" : ISODate("2021-10-17T10:36:53.362Z"),
			"lastHeartbeatRecv" : ISODate("2021-10-17T10:36:54.258Z"),
			"pingMs" : NumberLong(0),
			"lastHeartbeatMessage" : "",
			"syncSourceHost" : "server:28018",
			"syncSourceId" : 1,
			"infoMessage" : "",
			"configVersion" : 5,
			"configTerm" : 1
		}
	],
	"ok" : 1
}
```

+ **self**

这个字段只会出现在执行 `rs.status` 的成员信息中，在本例中是 28017 端口。

+ **stateStr**

用于描述服务器状态的字符串。

+ **uptime**

从成员可达一直到现在所经历的时间，单位是秒。如果是 `self` 成员，这个值是从它启动直到现在的时间。

+ **optimeDate**

每个成员的 `oplog` 中最后一个操作发生的时间（也就是操作被同步过来的时间）。注意，这里的状态是每个成员通过心跳报告上来的状态，所以 `optime` 跟实际时间可能会有几秒钟的偏差。

+ **lastHeartbeat**

当前服务器最后一次收到其他成员心跳的时间。如果网络故障或者当前服务器比较繁忙，这个时间可能会是 2 秒钟之前。

+ **pingMs**

心跳从当前服务器到达某个成员所花费的平均时间，可以根据这个字段选择从哪个成员进行同步。

有几个字段的信息是重复的：`state` 与 `stateStr` 都表示成员的状态，只是 `state` 的值是状态的内部表示法。

`health` 仅仅表示给定的服务器是否可达，而从 `state` 和 `stateStr` 也可以得到这样的信息（如果服务器不可达，它们的值会是 `UNKNOWN` 或者 `DOWN`）。

类似地，`optime` 和 `optimeDate` 的值也是相同的，只是表示方式不同：一个是用从新纪元开始的毫秒数表示的，另一个用一种更适合阅读的方式表示。

注意，这份报告是以执行 `rs.status()` 命令的成员的角度得出的：由于网络故障，这份报告可能不准确或者有些过时。

## 2 复制图谱

如果在备份节点上运行 `rs.status()`，输出信息中会有一个名为 `syncingTo` 的顶级字段，用于表示当前成员正在从哪个成员处进行复制。

如果在每个成员上运行 `replSetGetStatus` 命令，就可以弄清楚**复制图谱（replication graph）**。假设 `server1` 表示连接到 `server1` 的数据库连接，`server2` 表示连接到 `server2` 的数据库连接，以此类推，然后分别在这些连接上执行下面的命令：

```shell
$ server1.adminCommand({replSetGetStatus: 1})['syncingTo']
server0:27017
$ server2.adminCommand({replSetGetStatus: 1})['syncingTo']
server1:27017
$ server3.adminCommand({replSetGetStatus: 1})['syncingTo']
server1:27017
$ server4.adminCommand({replSetGetStatus: 1})['syncingTo']
server2:27017
```

所以，`server0` 是 `server1` 的同步源，`server1` 是 `server2` 和 `server3` 的同步源，`server2` 是 `server4` 的同步源。

`MongoDB` 根据 `ping` 时间选择同步源。一个成员向另一个成员发送心跳请求，就可以知道心跳请求所耗费的时间。`MongoDB` 维护着不同成员间请求的平均花费时间。选择同步源时，会选择一个离自己比较近而且数据比自己新的成员（所以，不会出现循环复制的问题，每个成员要么从主节点复制，要么从数据比它新的成员处复制）。

因此，如果在备份数据中心中添加一个新成员，它很可能会从与自己同在一个数据中心内的其他成员处复制，而不是从位于另一个数据中心的主节点处复制（这样可以减少网络流量），如图所示。

![image](https://github.com/TomatoZ7/notes-of-tz/blob/master/nosql/MongoDB/images/mongo_monitor_replica_1.jpg)

但是，**自动复制链（automatic replication chaining）**也有一些缺点：复制链越长，将写操作复制到所有服务器所花费的时间就越长。假设所有服务器都位于同一个数据中心内，然后，由于网络速度异常，新添加一个成员之后，`MongoDB` 的复制链如图所示。

![image](https://github.com/TomatoZ7/notes-of-tz/blob/master/nosql/MongoDB/images/mongo_monitor_replica_2.jpg)

通常不会发生这样的情况，只要出现这种状况，可以用 `replSetSyncFrom` 或者 `rs.syncForm()` 命令修改成员的复制源：

```shell
$ secondary.adminCommand({"replSetSyncForm": "server0:27017"})
```

## 3 复制循环

**如果复制链中出现了环，那么就成为发生了复制循环。**

例如，A 从 B 处同步数据，B 从 C 处同步数据，C 从 A 处同步数据，这就是一个复制循环。因为复制循环中的成员都不可能成为主节点，所以这些成员无法复制新的写操作，就会越来越落后。另一方面，**如果每个成员都是自动选取复制源，那么复制循环是不可能发生的**。

但是，使用 `replSetSyncFrom` 强制为成员设置复制源时，就可能会出现复制循环。在手动修改成员的复制源时，应该仔细查看 `rs.status()` 的输出信息，避免造成复制循环。当用 `replSetSyncFrom` 为成员指定一个并不比它领先的成员作为复制源时，系统会给出警告，但仍然允许这么做。

## 4 禁用复制链

当一个备份节点从另一个备份节点（而不是主节点）复制数据时，就会形成复制链。前面说过，成员会自动选择其他成员作为复制源。

可以禁用复制链，强制要求每个成员都从主节点进行复制，只需要将 `allowChaining` 设置为 `false` 即可（如果不指定这个选项，默认是 `true`）：

```shell
$ var config = rs.config()
$ config.settings = config.settings || {}
$ config.settings.allowChaining = false
$ re.reconfig(config)
```

如果主节点变得不可用，那么各个成员就会从其他备份节点处复制数据。

## 5 计算延迟

跟踪复制情况的一个重要指标是备份节点与主节点之间的延迟程度。**延迟（lag）是指备份节点相对于主节点的落后程度，是主节点最后一次操作的时间戳与备份节点最后一次操作的时间戳的差。**

可以使用 `rs.status()` 查看成员的复制状态，也可以通过在主节点上执行 `db.printReplicationInfo()`（这个命令的输出信息中包含 `oplog` 相关信息），或者在备份节点上执行 `db.printSecondaryReplicationInfo()` 快速得到一份摘要。注意，这两个都是 `db` 的函数，而不是 `rs` 的。

`db.printReplicationInfo` 的输出中包括主节点的 `oplog` 信息：

```shell
$ db.printReplicationInfo()
configured oplog size:   2503.301513671875MB
log length start to end: 950409secs (264hrs)
oplog first event time:  Thu Oct 07 2021 22:52:27 GMT+0800 (CST)
oplog last event time:   Mon Oct 18 2021 22:52:36 GMT+0800 (CST)
now:                     Mon Oct 18 2021 22:52:36 GMT+0800 (CST)
```

上面的输出信息中包含了 `oplog` 的大小，以及 `oplog` 中包含的操作的时间范围。在本例中，`oplog` 的大小大约是 2503MB，包含大约 12 天的操作。

`oplog` 中第一条操作与最后一条操作的时间差就是操作日志的长度。

在备份节点上运行 `db.printSecondaryReplicationInfo()`，可以得到当前成员的复制源，以及当前成员相对复制源的落后程度等信息：

```shell
$ db.printSecondaryReplicationInfo()
source: server-0:28018
	syncedTo: Tue Oct 19 2021 21:33:29 GMT+0800 (CST)
	0 secs (0 hrs) behind the primary 
source: server-0:28019
	syncedTo: Tue Oct 19 2021 21:33:29 GMT+0800 (CST)
	0 secs (0 hrs) behind the primary
```

注意，副本集成员的延迟是**相对于主节点**来说的，而不是表示需要多长时间才能更新到最新。在一个写操作非常少的系统中，有可能会造成延迟过大的幻觉。假设一小时执行一次写操作。刚刚执行完这次写操作之后，复制之前，备份节点会落后于主节点一小时。但是，只需要几毫秒时，备份节点就可以追上主节点。当监控低吞吐量的系统时，这个值可能会造成迷惑。

## 6 调整 oplog 大小

如果要增加 `oplog` 大小，可以按照如下步骤：

1. 如果当前服务器是主节点，让它退位，以便让其他成员的数据能够尽快更新到与它一致。

2. 关闭当前服务器。

3. 将当前服务器以单机模式启动。

4. 临时将 `oplog` 中的最后一条 `insert` 操作保存到其他集合中：

```shell
$ use local
# {"op": "i"} 用于查找最后一条 insert 操作
$ var cursor = db.oplog.rs.find({"op": "i"})

$ var lastInsert = cursor.sort({"$natural": -1}).limit(1).next()
$ db.tempLastOp.save(lastInsert)

# 确保保存成功，非常重要
$ db.tempLastOp.findOne()
```

5. 删除当前的 `oplog`

```shell
$ db.oplog.rs.drop()
```

6. 创建一个新的 `oplog`：

```shell
$ db.createCollection("oplog.rs", {"capped": true, "size": 10000})
```

7. 将最后一条记录写回 `oplog`：

```shell
$ var temp = db.tempLastOp.findOne()
$ db.oplog.rs.insert(temp)
# 确保插入成功，否则把当前服务器添加到副本集之后，它会删除所有数据，然后重新进行一次完整同步。
$ db.oplog.rs.findOne()
```

8. 最后，将当前服务器作为副本及成员重新启动。由于这时它的 `oplog` 只有一条记录，所以在一段时间内无法知道 `oplog` 的真实长度。另外，这个服务器现在也并不适合作为其他成员的复制源。

通常不应该减小 `oplog` 的大小：即使 `oplog` 可能会有几个月那么长，但是通常总是有足够的硬盘空间来保存 `oplog`，`oplog` 并不会占用任何珍贵的资源（比如 CPU 或 RAM）。

## 7 从延迟备份节点中恢复

假设有人不小心删除了一个数据库，幸好你有一个延迟备份节点。现在，需要放弃其他成员的数据，明确将延迟备份节点指定为数据源。

下面介绍最简单的方法。

1. 关闭所有其他成员。

2. 删除其他成员数据目录中的所有数据。确保每个成员（除了延迟备份节点）的数据目录都是空的。

3. 重启所有成员，然后它们会自动从延迟备份节点中复制数据。

这种方式非常简单。但是，在其他成员完成初始化同步之前，副本集中将**只有一个成员可用（延迟备份节点）**而且这个成员很可能会过载。

根据数据量的不同，第二种方式可能更好，也可能更差。

1. 关闭所有成员，包括延迟备份节点。

2. 删除其他成员（除了延迟备份节点）的数据目录。

3. 将延迟备份节点的数据文件复制到其他服务器。

4. 重启所有成员。

注意，这样会导致所有服务器都与延迟备份节点拥有同样大小的 `oplog`，这可能不是你想要的。

## 8 创建索引

如果向主节点发送创建索引的命令，主节点会正常创建索引，然后备份节点在复制“创建索引”操作时也会创建索引。这是最简单的创建索引的方式，但是创建索引是一个需要消耗大量资源的操作，可能会导致成员不可用。如果所有备份节点都在同一时间开始创建索引，那么几乎所有成员都会不可用，一直到索引创建完成。

因此，可能你会希望每次只在一个成员上创建索引，以降低对应用程序的影响。如果要这么做，有下面几个步骤。

1. 关闭一个备份节点服务器。

2. 将这个服务器以单机模式启动。

3. 在单机模式下创建索引。

4. 索引创建完成之后，将服务器作为副本集成员重新启动。

5. 对副本集的每个备份节点重复 1 ~ 4 步。

现在副本集的每个成员（除了主节点）都已经成功创建了索引。现在有 2 个选择，根据实际情况选择一个对生产系统影响最小的方式。

1. 在主节点上创建索引。如果系统会有一段负载比较小的“空闲期”，那会是非常好的创建索引的时机。也可以修改读取首选项，在主节点创建索引期间，将读操作发送到备份节点上。

主节点创建索引之后，备份节点仍然会复制这个操作，但是由于备份节点中已经有了同样的索引，实际上不会再次创建索引。

2. 让主节点退化为备份节点，对这个服务器执行上面 4 步。这时就会发生故障转移，这期间会选举出新的主节点。索引创建完成之后，可以重新将服务器添加到副本集。

注意，可以使用这种技术为某个备份节点创建**与其他成员不同的索引**。这种方式在做离线数据处理时非常有用，但是，**如果某个备份节点的索引与其他成员不同，那么它永远不能成为主节点：应该将它的优先级设为 0。**

如果**要创建唯一索引**，需要先确保主节点中没有被插入重复的数据，或者应该首先为主节点创建唯一索引。否则，可能会有重复数据插入主节点，这会导致备份节点复制时出错，如果遇到这样的错误，备份节点会将自己关闭。你不得不以单机模式启动这台服务器，删除唯一索引，然后重新将其加入副本集。

## 9 在预算有限的情况下进行复制

如果预算有限，不能使用多台高性能服务器，可以考虑将备份节点只用于灾难恢复，这样的备份节点不需要太大的 `RAM` 和太好的 `CPU`，也不需要太高的磁盘 IO。这样，始终将高性能服务器作为主节点，比较便宜的服务器只用于备份，不处理任何客户端请求（将客户端配置为将全部读请求发送到主节点）。对于这样的备份节点，应该设置这些选项。

+ `priority: 0`

优先级为 0 的备份节点永远不会成为主节点。

+ `hidden: true`

将备份节点设为隐藏，客户端就无法将读请求发送给它了。

+ `buildIndexes: false`

这个选项是可选的，如果在备份节点上创建索引的话，会极大地降低备份节点的性能。如果不在备份节点上创建索引，所以从备份节点中恢复数据之后，需要重新创建索引。

+ `votes: 0`

在只有 2 台服务器的情况下，如果将备份节点的投票数设为 0，那么当备份节点挂掉之后，主节点仍然会一直是主节点，不会因为达不到“大多数”的要求而退位。如果还有第三台服务器（即使它是你的应用服务器），那么应该在第三台服务器上运行一个仲裁者成员，而不是将第三台服务器的投票数量设为 0。

## 10 主节点如何跟踪延迟

作为其他成员的同步源的成员会维护一个名为 `local.slaves` 的集合，这个集合中保存着所有正从当前成员进行数据同步的成员，以及每个成员的数据新旧程度。如果使用 `w` 参数执行查询，`MongoDB` 会根据这些信息确定是否有足够多、足够新的备份节点可以用来处理查询。

`local.slave` 集合实际上时内存中数据结构的“回声”，所以其中的数据可能会有几秒钟的延迟：

```shell
$ db.slaves.find()
{ "_id" : ObjectId("4c1287178e00e93d1858567c"), "host" : "10.4.1.100",
	"ns" : "local.oplog.rs", "syncedTo" : { "t" : 1276282710000, "i" : 1 } }
{ "_id" : ObjectId("4c128730e6e5c3096f40e0de"), "host" : "10.4.1.101",
	"ns" : "local.oplog.rs", "syncedTo" : { "t" : 1276282710000, "i" : 1 } }
```

每个服务器的 `_id` 字段非常重要：它是所有正在从当前成员进行数据同步的服务器的标识符。连接到一个成员，然后查询 `local.me` 集合就可以知道一个成员的标识符：

```shell
$ db.me.findOne()
{ "_id" : ObjectId("50e6edb517c789e46695212f"), "host" : "server-1" }
```

如果多台服务器拥有相同的 `_id`，可以依次登录到每台服务器，删除 `local.me` 集合，然后重新启动 `mongod`。启动时，`mongod` 会使用新的 `_id` 重新生成 `local.me` 集合。

如果服务器的地址发生了改变（`_id` 没有变但是主机名变了），可能会在本地数据库的日志中看到键重复异常（duplicate key exception）。遇到这种情况时，删除 `local.slaves` 集合即可。

`mongod` 不会清理 `local.slaves` 集合，所以，它可能会列出某个几个月之前就不再把该成员作为同步源的服务器（或者是已经不在副本集内的成员）。由于 `MongoDB` 只是把这个集合用于报告复本集状态，所以这个集合中的过时数据并不会有什么影响。如果你觉得这个集合中的旧数据会造成困惑或者是过于混乱，可以将整个集合删除。几秒钟之后，如果有新的服务器将当前成员作为复制源的话，这个集合就会重新生成。

如果备份节点之间形成了复制链，你可能会注意到某个特定的服务器在主节点的 `local.slaves` 集合中有多个文档。这是因为，每个备份节点都会将复制请求转发给它的复制源，这样柱节点就能够知道每个备份节点的同步源。这成为**影同步（ghost syncs），因为这些请求并不会要求进行数据同步，只是把没个备份节点的同步源报告给主节点。

`local` 数据库只用于位于复制相关信息，它并不会被复制。因此，如果希望某些数据只存在于特定的机器上，可以将这些数据保存在 `local` 数据库的集合中。