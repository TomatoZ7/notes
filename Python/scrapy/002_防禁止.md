# 防禁止

我们在运行爬虫的时候，如果爬取的网页较多，经常会遇到这种问题。因为现在很多网站都有相应的反爬虫机制，避免爬虫的恶意爬取。

所以，当我们要爬取大量网页的时候，很可能会受到对方服务器的限制，从而被禁止，显然，这不是我们想要的结果。

那么在 `Scrapy` 爬虫项目中，我们应该如何应对这些反爬虫机制呢？

在 `Scrapy` 项目中，主要可以通过以下方法来避免被禁止：

1. 禁止Cookie；

2. 设置下载延时；

3. 使用IP池；

4. 使用用户代理池；

5. 其他方法，比如进行分布式爬取等。

## 1.禁止 Cookie

有的网站会通过用户的 `Cookie` 信息对用户进行识别和分析，此时我们可以通过禁用本地 `Cookies` 信息让对方网站无法识别出我们的会话信息，从而无法禁止我们的爬取。

打开对应的 `Scrapy` 爬虫项目中的 `settings.py` 文件，可以发现文件中有以下两行代码：

```py
# Disable cookies (enabled by default)
# COOKIES_ENABLED = False
```

这两行代码都是被注释的状态，这两行代码就是设置禁止使用 `Cookie` 的代码，我们若要禁用 `Cookie`，只需要把 `# COOKIES_ENABLED = False` 的注释去掉即可，修改为如下代码：

```py
# Disable cookies (enabled by default)
COOKIES_ENABLED = False
```

这样，就可以禁用本地 `Cookie`，让那些通过用户的 `Cookie` 信息对用户进行识别和分析的网站无法识别我们，即无法禁止我们爬取。

## 2.设置下载延时

有的网站会通过我们对网页的访问（爬取）频率进行分析，如果爬取频率过快，则判断为爬虫自动爬取行为，识别后对我们进行相应限制，比如禁止我们再爬取该服务器上的网页等。

对于这一类网站，我们只需要控制一下爬行时间间隔即可。

在 `Scrapy` 爬虫项目中，我们可以直接在对应的 `Scrapy` 爬虫项目中的 `settings.py` 文件中进行相应的设置即可。打开 `settings.py` 文件，会发现有如下几行代码：

```py
# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
```

如果要设置网页下载时间间隔，通过这几行代码配置即可。

上面代码中的前 3 行为配置说明信息，第 4 行 `#DOWNLOAD_DELAY = 3` 为设置时间间隔的具体代码，解除这一行的注释即可实现下载延时的配置，其中 3 代表 3 秒，如果想将爬虫下载网页的时间间隔设置为 0.5 秒，将对应的值改为 0.5 即可。

比如，我们想将爬虫下载网页的时间间隔设置为 0.7 秒，可以将上述代码改为：

```py
# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
DOWNLOAD_DELAY = 0.7
```

## 3.使用 IP 池

有的网站会对用户的 IP 进行检测，如果同一个 IP 在短时间内对自己服务器上的网页进行大量的爬取，那么可以初步判定为网络爬虫的自动爬取行为，如有必要，该网站可以对该 IP 进行封禁。

作为爬虫方，如果我们的 IP 被封禁了，就需要更换 IP，对于普通用户来说，IP 资源可能会有限，那么怎么样才能有较多的 IP 呢？

利用不同的代理服务器我们可以获得不同的 IP，所以此时我们可以获取多个代理服务器，将这些代理服务器的 IP 组成一个 IP 池，爬虫每次对网页进行爬取的时候，可以随机选择 IP 池中的一个 IP 进行。

此时，我们可以为 `Scrapy` 爬虫项目建立一个下载中间件，在下载中间件中设置好 IP 选择规则，在 `settings.py` 设置文件中配置好下载中间件，并配置好 IP 池。

具体的代理 IP 可在网上查找或购买。