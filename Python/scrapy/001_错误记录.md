# scrapy 开发过程中的错误记录

## 1 

运行 `scrapy crawl spider_name` 之后错误提示如下：

```py
Traceback (most recent call last):
  File "xxx/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 44, in process_request
    return (yield download_func(request=request, spider=spider))
```

原因是请求头没有设置 `User-Agent`，设置即可。

### 解决：

`scrapy` 有 2 种设置请求头的方式：

1. `setting.py` 文件设置：

```py
# Crawl responsibly by identifying yourself (and your website) on the user-agent
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) ' \
             'Chrome/96.0.4664.55 Safari/537.36 '
```

2. 随机 `User-Agent`

首先在 `middlewares.py` 创建 `RandomUserAgentMiddleware` 类：

```py
class RandomUserAgentMiddleware():
    def __init__(self):
        self.user_agent = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/96.0.4664.55 Safari/537.36',
            'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.2 (KHTML, like Gecko) Chrome/22.0.1216.0 Safari/537.2'
        ]

    def process_request(self, request, spider):
        request.headers['User-Agent'] = random.choice(self.user_agent)
```

要使其生效，还需在 `settings.py` 配置：

```py
# Enable or disable downloader middlewares
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
DOWNLOADER_MIDDLEWARES = {
   'scrapy_demo.middlewares.RandomUserAgentMiddleware': 543
}
```

## 2 xlwt 重复操作单元格

使用 `xlwt` 库重复操作单个单元格时会报错：

```py
Exception: Attempt to overwrite cell
```

### 解决

```py
sheet.write(data, cell_overwrite_ok=True)
```